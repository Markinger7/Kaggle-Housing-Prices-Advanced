{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Prices Models\n",
    "> This notebook build use the information from the exploration step to perform the preprocessing and build the ML Models\n",
    "\n",
    "### Goal\n",
    "> predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable.\n",
    "\n",
    "This notebook summarizes the whole preprocessing steps, includes feature selection based on visualizations in the preparation notebook, tests of various machine learning algorithms and a basic blended ml model. With that it is used as a basis to explore feature engineering, GridSearch to find the best hyperparameters for each machine learning algorithm, implement more advanced blending or even stacking methods to enhance the predictive sore.\n",
    "\n",
    "\n",
    "### Content\n",
    "- [Preprocessing](#pre)\n",
    "- [Modeling](#modeling)\n",
    "- [Use Models to make predictions](#pred)\n",
    "\n",
    "### Data\n",
    "- <a href='https://www.kaggle.com/c/house-prices-advanced-regression-techniques'>Link</a> to Kaggle competition \n",
    "- <a href='https://amstat.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627#.X1ZIXy337UI'>Paper</a> on the data: \n",
    "\n",
    "\n",
    "### Score of final model\n",
    "- blended model with XGBRegressor, LGBMRegressor and Ridge Regression (without any hyperparameter tuning or feature engineering): \n",
    "    - Kaggle Score: 0.13696\n",
    "    - top 46%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ml related imports\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "\n",
    "\n",
    "# silence settingWithCopyWarning\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 81), (1459, 80))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pre'></a>\n",
    "### Preprocessing\n",
    "- remove missing values\n",
    "- transform `MSSubClass` and `MoSold` to a categorical feature\n",
    "- drop 'unimportant' categorical features\n",
    "- preprocess categorical features\n",
    "- split all_data into train and test again\n",
    "- transform numerical features\n",
    "    - log transform `SalePrice`\n",
    "    - drop numerical features that have a correlation below 0.3\n",
    "    - drop outliers\n",
    "- Identify columns to drop for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre = train.copy()\n",
    "test_pre = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove missing values\n",
    "> For this I will concat the train and test set. Since I am doing only basic missing value imputation (mean, median, and mode), the problem of data leakage will be minimized. When using more advanced methods like KNN-imputation or missForest pone should definitely perform those one the dataset seperately since information will leak into to train set from the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train_pre, test_pre], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a lot of the missing values are just encodings for the instance that a specific feaure isn't available\n",
    "# list of features with worng encodign for NA\n",
    "feature_NA = ['Alley', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', \n",
    "              'MiscFeature', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\n",
    "\n",
    "# assign NA to None to indicate the lack of a certain feature\n",
    "all_data[feature_NA] = all_data[feature_NA].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imute missing categorical features mostly with the mode\n",
    "all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n",
    "all_data['Utilities'] = all_data['Utilities'].fillna(all_data['Utilities'].mode()[0])\n",
    "all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n",
    "all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n",
    "all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n",
    "all_data['MasVnrType'] = all_data['MasVnrType'].fillna('None')\n",
    "all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n",
    "all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n",
    "all_data['Functional'] = all_data['Functional'].fillna(all_data['Functional'].mode()[0])\n",
    "all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imput missing numerical features (most numercial had only 1-2 missing values in that case I just imputet 0)\n",
    "all_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)\n",
    "all_data['BsmtFinSF1'] = all_data['BsmtFinSF1'].fillna(0)\n",
    "all_data['BsmtFinSF2'] = all_data['BsmtFinSF2'].fillna(0)\n",
    "all_data['BsmtUnfSF'] = all_data['BsmtUnfSF'].fillna(0)\n",
    "all_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].fillna(0)\n",
    "all_data['BsmtFullBath'] = all_data['BsmtFullBath'].fillna(0)\n",
    "all_data['BsmtHalfBath'] = all_data['BsmtHalfBath'].fillna(0)\n",
    "all_data['GarageCars'] = all_data['GarageCars'].fillna(0)\n",
    "all_data['GarageArea'] = all_data['GarageArea'].fillna(0)\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].fillna(0)\n",
    "# Neighorhood should impact the size of of street connected to the property\n",
    "# code from https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform `MSSubClass` and `MoSold` to a categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['MSSubClass'] = all_data['MSSubClass'].astype('str')\n",
    "\n",
    "# create dict to assign categories\n",
    "MoSold_dict = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "all_data['MoSold'] = all_data['MoSold'].map(MoSold_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop 'unimportant' categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame with relevant categorical feats\n",
    "all_cat_feats = list(all_data.select_dtypes('object'))\n",
    "# do the same for the numerical features befor processing categorical features and creating dummies\n",
    "all_num_feats = list(all_data.select_dtypes(exclude='object'))\n",
    "\n",
    "important_cat_feats = ['Neighborhood', 'MSZoning', 'MSSubClass', 'HouseStyle', 'Foundation', 'ExterQual', 'KitchenQual', \n",
    "                       'GarageQual', 'CentralAir', 'HeatingQC', 'BsmtQual']\n",
    "\n",
    "cat_feats_drop = list(set(all_cat_feats) - set(important_cat_feats))\n",
    "\n",
    "all_data.drop(columns=cat_feats_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greate dummies for real categorical features with drop_first=True to reduce multicollinearity\n",
    "list_dummies = ['MSSubClass', 'MSZoning', 'Neighborhood', 'HouseStyle', 'Foundation']\n",
    "df_dummy = pd.get_dummies(all_data[list_dummies], drop_first=True)\n",
    "all_data.drop(columns=list_dummies, inplace=True)\n",
    "all_data = pd.concat([all_data, df_dummy], axis=1)\n",
    "\n",
    "# converte categorical to ordinal features and use them as numeric input in the model\n",
    "list_ord = ['ExterQual', 'KitchenQual', 'GarageQual', 'HeatingQC', 'BsmtQual']\n",
    "map_dict_ord = {'None': 0, 'Po': 1, 'Fa': 2, 'TA':3, 'Gd':4, 'Ex': 5}\n",
    "for ord_ in list_ord:\n",
    "    all_data[ord_] = all_data[ord_].map(map_dict_ord)\n",
    "\n",
    "# convert categorical to binary\n",
    "all_data['CentralAir'] = all_data['CentralAir'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split all_data into train and test again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split all_data in train and test to perform more preprocessing and feature engineering speratly (prevent data leakage)\n",
    "train_pre = all_data.loc[:train.shape[0]-1]\n",
    "test_pre = all_data.loc[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 97), (1459, 97))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pre.shape, test_pre.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Log transform `SalePrice` and save in target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.log(train_pre.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### drop numerical features that have a correlation below 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create matrix\n",
    "corr_matrix = train_pre[all_num_feats]\n",
    "corr_matrix = corr_matrix.corr()\n",
    "corr_matrix = corr_matrix['SalePrice']\n",
    "\n",
    "# filter matrix: drop num features with corrleation below 0.3\n",
    "num_feats_drop = corr_matrix[corr_matrix < 0.3].index\n",
    "train_pre.drop(columns=num_feats_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### drop outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre = train_pre.drop(train_pre[(train_pre['GrLivArea']>4000) & (train_pre['SalePrice']<300000)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add target to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre['SalePrice_log'] = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify columns to drop for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save id \n",
    "Id = test_pre['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features to drop\n",
    "to_drop = list(set(test_pre) - set(train_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre.drop(columns='SalePrice', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeling'></a>\n",
    "### Modeling\n",
    "- make x_train and y_train \n",
    "- define Cross Validation\n",
    "- initiate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make x_train and y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use train set to make x_train and y_train\n",
    "x_train = train_pre.drop(columns=['SalePrice_log', 'SalePrice'])\n",
    "y_train = train_pre['SalePrice_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1458, 78), (1458,), (1459, 78))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, test_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if x_train and test_pre are identical\n",
    "list(set(x_train) - set(test_pre)), list(set(test_pre) - set(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation\n",
    "> define cross validation method to evaluate different machine learning models on the training data before using a model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code: https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, x_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "model_xgb = XGBRegressor()\n",
    "# LightGBM\n",
    "model_lgb = LGBMRegressor(objective='regression')\n",
    "# Lasso Regression\n",
    "model_lasso = Lasso()\n",
    "# Ridge Regression\n",
    "model_ridge = Ridge()\n",
    "\n",
    "\n",
    "# list of models for cross validation\n",
    "models_list = [model_xgb, model_lgb, model_lasso, model_ridge]\n",
    "model_names = ['model_xgb', 'model_lgb', 'model_lasso', 'model_ridge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_xgb rmsle score:\n",
      "0.14445967712041968\n",
      "##############################\n",
      "model_lgb rmsle score:\n",
      "0.13746055595980713\n",
      "##############################\n",
      "model_lasso rmsle score:\n",
      "0.17146718486654478\n",
      "##############################\n",
      "model_ridge rmsle score:\n",
      "0.1244083497141513\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "for model, name in zip(models_list, model_names):\n",
    "    print(name + ' rmsle score:')\n",
    "    print(np.mean(rmsle_cv(model)))\n",
    "    print('#'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> basic Ridge Regressions seems to be the best out of the selected basis models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use models on test data\n",
    "- use single model\n",
    "- basic blended model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use LightGBM to make a submission\n",
    "> used the following code to make predictions for multiple algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=100, n_jobs=-1, num_leaves=31,\n",
       "              objective='regression', random_state=None, reg_alpha=0.0,\n",
       "              reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "              subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train on train data\n",
    "model_lgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_preds = model_lgb.predict(test_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['Id'] = Id\n",
    "# transform log of SalePrice\n",
    "submission['SalePrice'] = np.exp(lgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('sub_lgb_basis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>121332.313427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>154402.770147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>187984.471281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>182457.513648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>196060.735397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>73066.162924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>83065.032787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>169410.979017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>108783.834550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>229765.736187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  121332.313427\n",
       "1     1462  154402.770147\n",
       "2     1463  187984.471281\n",
       "3     1464  182457.513648\n",
       "4     1465  196060.735397\n",
       "...    ...            ...\n",
       "1454  2915   73066.162924\n",
       "1455  2916   83065.032787\n",
       "1456  2917  169410.979017\n",
       "1457  2918  108783.834550\n",
       "1458  2919  229765.736187\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Model Blending\n",
    "> use XGBRegressor, LGBMRegressor and Ridge Regression to make a prediction, and calculate the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "      normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb.fit(x_train, y_train)\n",
    "model_lgb.fit(x_train, y_train)\n",
    "model_ridge.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_preds =model_xgb.predict(test_pre)\n",
    "lgb_preds = model_lgb.predict(test_pre)\n",
    "ridge_preds =model_ridge.predict(test_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (xgb_preds + lgb_preds + ridge_preds)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['Id'] = Id\n",
    "# transform log of SalePrice\n",
    "submission['SalePrice'] = np.exp(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('blended_model_basis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>118955.879161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>157738.755660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>185493.887521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>191904.085786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>198065.073784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>73897.248834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>84286.280698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>172593.555022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>110948.817135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>223699.428286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  118955.879161\n",
       "1     1462  157738.755660\n",
       "2     1463  185493.887521\n",
       "3     1464  191904.085786\n",
       "4     1465  198065.073784\n",
       "...    ...            ...\n",
       "1454  2915   73897.248834\n",
       "1455  2916   84286.280698\n",
       "1456  2917  172593.555022\n",
       "1457  2918  110948.817135\n",
       "1458  2919  223699.428286\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
