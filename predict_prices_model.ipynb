{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Prices Models\n",
    "> This notebook build use the information from the exploration step to perform the preprocessing and build the ML Models\n",
    "\n",
    "### Goal\n",
    "> predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable.\n",
    "\n",
    "This notebook summarizes the whole preprocessing steps, includes feature selection based on visualizations in the preparation notebook, tests of various machine learning algorithms and a basic blended ml model. With that it is used as a basis to explore feature engineering, GridSearch to find the best hyperparameters for each machine learning algorithm, implement more advanced blending or even stacking methods to enhance the predictive sore.\n",
    "\n",
    "\n",
    "### Content\n",
    "- [Preprocessing](#pre)\n",
    "- [Modeling](#modeling)\n",
    "- [Use Models to make predictions](#pred)\n",
    "\n",
    "### Data\n",
    "- <a href='https://www.kaggle.com/c/house-prices-advanced-regression-techniques'>Link</a> to Kaggle competition \n",
    "- <a href='https://amstat.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627#.X1ZIXy337UI'>Paper</a> on the data: \n",
    "\n",
    "\n",
    "### Score of final model\n",
    "- blended model with CatBoost, LGBMRegressor and Ridge Regression (without any hyperparameter tuning or feature engineering): \n",
    "    - Kaggle Score: 0.13608\n",
    "    - top 44%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ml related imports\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV, train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# silence settingWithCopyWarning\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 81), (1459, 80))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pre'></a>\n",
    "### Preprocessing\n",
    "- remove missing values\n",
    "- transform `MSSubClass` and `MoSold` to a categorical feature\n",
    "- drop 'unimportant' categorical features\n",
    "- preprocess categorical features\n",
    "- split all_data into train and test again\n",
    "- transform numerical features\n",
    "    - log transform `SalePrice`\n",
    "    - drop numerical features that have a correlation below 0.3\n",
    "    - drop outliers\n",
    "- Identify columns to drop for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre = train.copy()\n",
    "test_pre = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove missing values\n",
    "> For this I will concat the train and test set. Since I am doing only basic missing value imputation (mean, median, and mode), the problem of data leakage will be minimized. When using more advanced methods like KNN-imputation or missForest pone should definitely perform those one the dataset seperately since information will leak into to train set from the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train_pre, test_pre], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a lot of the missing values are just encodings for the instance that a specific feaure isn't available\n",
    "# list of features with worng encodign for NA\n",
    "feature_NA = ['Alley', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', \n",
    "              'MiscFeature', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\n",
    "\n",
    "# assign NA to None to indicate the lack of a certain feature\n",
    "all_data[feature_NA] = all_data[feature_NA].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imute missing categorical features mostly with the mode\n",
    "all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n",
    "all_data['Utilities'] = all_data['Utilities'].fillna(all_data['Utilities'].mode()[0])\n",
    "all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n",
    "all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n",
    "all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n",
    "all_data['MasVnrType'] = all_data['MasVnrType'].fillna('None')\n",
    "all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n",
    "all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n",
    "all_data['Functional'] = all_data['Functional'].fillna(all_data['Functional'].mode()[0])\n",
    "all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imput missing numerical features (most numercial had only 1-2 missing values in that case I just imputet 0)\n",
    "all_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)\n",
    "all_data['BsmtFinSF1'] = all_data['BsmtFinSF1'].fillna(0)\n",
    "all_data['BsmtFinSF2'] = all_data['BsmtFinSF2'].fillna(0)\n",
    "all_data['BsmtUnfSF'] = all_data['BsmtUnfSF'].fillna(0)\n",
    "all_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].fillna(0)\n",
    "all_data['BsmtFullBath'] = all_data['BsmtFullBath'].fillna(0)\n",
    "all_data['BsmtHalfBath'] = all_data['BsmtHalfBath'].fillna(0)\n",
    "all_data['GarageCars'] = all_data['GarageCars'].fillna(0)\n",
    "all_data['GarageArea'] = all_data['GarageArea'].fillna(0)\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].fillna(0)\n",
    "# Neighorhood should impact the size of of street connected to the property\n",
    "# code from https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform `MSSubClass` and `MoSold` to a categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['MSSubClass'] = all_data['MSSubClass'].astype('str')\n",
    "\n",
    "# create dict to assign categories\n",
    "MoSold_dict = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "all_data['MoSold'] = all_data['MoSold'].map(MoSold_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell to define which categorical features should be included "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame with relevant categorical feats\n",
    "all_cat_feats = list(all_data.select_dtypes('object'))\n",
    "# do the same for the numerical features befor processing categorical features and creating dummies\n",
    "all_num_feats = list(all_data.select_dtypes(exclude='object'))\n",
    "\n",
    "important_cat_feats = ['Neighborhood', 'MSZoning', 'MSSubClass', 'HouseStyle', 'Foundation', 'ExterQual', 'KitchenQual', \n",
    "                       'GarageQual', 'CentralAir', 'HeatingQC', 'BsmtQual']\n",
    "\n",
    "cat_feats_drop = list(set(all_cat_feats) - set(important_cat_feats))\n",
    "\n",
    "all_data.drop(columns=cat_feats_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greate dummies for real categorical features with drop_first=True to reduce multicollinearity\n",
    "list_dummies = ['MSSubClass', 'MSZoning', 'Neighborhood', 'HouseStyle', 'Foundation']\n",
    "df_dummy = pd.get_dummies(all_data[list_dummies], drop_first=True)\n",
    "all_data.drop(columns=list_dummies, inplace=True)\n",
    "all_data = pd.concat([all_data, df_dummy], axis=1)\n",
    "\n",
    "# converte categorical to ordinal features and use them as numeric input in the model\n",
    "list_ord = ['ExterQual', 'KitchenQual', 'GarageQual', 'HeatingQC', 'BsmtQual']\n",
    "map_dict_ord = {'None': 0, 'Po': 1, 'Fa': 2, 'TA':3, 'Gd':4, 'Ex': 5}\n",
    "for ord_ in list_ord:\n",
    "    all_data[ord_] = all_data[ord_].map(map_dict_ord)\n",
    "\n",
    "# convert categorical to binary\n",
    "all_data['CentralAir'] = all_data['CentralAir'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "- total square feet should be important when predicting the house price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split all_data into train and test again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split all_data in train and test to perform more preprocessing and feature engineering speratly (prevent data leakage)\n",
    "train_pre = all_data.loc[:train.shape[0]-1]\n",
    "test_pre = all_data.loc[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 97), (1459, 97))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pre.shape, test_pre.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Log transform `SalePrice` and save in target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.log(train_pre.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### drop numerical features that have a correlation below 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create matrix\n",
    "corr_matrix = train_pre[all_num_feats]\n",
    "corr_matrix = corr_matrix.corr()\n",
    "corr_matrix = corr_matrix['SalePrice']\n",
    "\n",
    "# filter matrix: drop num features with corrleation below 0.3\n",
    "num_feats_drop = corr_matrix[corr_matrix < 0.2].index\n",
    "train_pre.drop(columns=num_feats_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### drop outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre = train_pre.drop(train_pre[(train_pre['GrLivArea']>4000) & (train_pre['SalePrice']<300000)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add target to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre['SalePrice_log'] = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify columns to drop for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save id \n",
    "Id = test_pre['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features to drop\n",
    "to_drop = list(set(test_pre) - set(train_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre.drop(columns='SalePrice', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeling'></a>\n",
    "### Modeling\n",
    "- make x_train and y_train \n",
    "- define Cross Validation\n",
    "- initiate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make x_train and y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use train set to make x_train and y_train\n",
    "x_train = train_pre.drop(columns=['SalePrice_log', 'SalePrice'])\n",
    "y_train = train_pre['SalePrice_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1458, 83), (1458,), (1459, 83))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, test_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if x_train and test_pre are identical\n",
    "list(set(x_train) - set(test_pre)), list(set(test_pre) - set(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation\n",
    "> define cross validation method to evaluate different machine learning models on the training data before using a model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code: https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmsle= np.sqrt(-cross_val_score(model, x_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmsle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "model_xgb = XGBRegressor()\n",
    "# LightGBM\n",
    "model_lgb = LGBMRegressor()\n",
    "# CatBoost\n",
    "model_cat = CatBoostRegressor(verbose=False)\n",
    "# Lasso Regression\n",
    "model_lasso = Lasso()\n",
    "# Ridge Regression\n",
    "model_ridge = Ridge()\n",
    "\n",
    "\n",
    "\n",
    "# list of models for cross validation\n",
    "models_list = [model_xgb, model_lgb, model_cat, model_lasso, model_ridge]\n",
    "model_names = ['model_xgb', 'model_lgb', 'model_cat', 'model_lasso', 'model_ridge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_xgb rmsle score:\n",
      "0.1436501915261411\n",
      "##############################\n",
      "model_lgb rmsle score:\n",
      "0.1349507619433274\n",
      "##############################\n",
      "model_cat rmsle score:\n",
      "0.12534051694673637\n",
      "##############################\n",
      "model_lasso rmsle score:\n",
      "0.16829118978283888\n",
      "##############################\n",
      "model_ridge rmsle score:\n",
      "0.12196660786631104\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "for model, name in zip(models_list, model_names):\n",
    "    print(name + ' rmsle score:')\n",
    "    print(np.mean(rmsle_cv(model)))\n",
    "    print('#'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use models to make predictions\n",
    "- check the mean absolute error of each model\n",
    "- use single model\n",
    "- basic blended model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_xgb mean absolut error:\n",
      "17200.763550311203\n",
      "model_lgb mean absolut error:\n",
      "16550.233140546134\n",
      "model_cat mean absolut error:\n",
      "14852.295345761135\n",
      "model_lasso mean absolut error:\n",
      "21081.394337767597\n",
      "model_ridge mean absolut error:\n",
      "15000.05024620343\n"
     ]
    }
   ],
   "source": [
    "# split the data to evaluate each model\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(x_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "for model, name in zip(models_list, model_names):\n",
    "    model.fit(X_train_s, y_train_s)\n",
    "    preds = model.predict(X_test_s)\n",
    "    print(name + ' mean absolut error:')\n",
    "    print(metrics.mean_absolute_error(np.exp(y_test_s), np.exp(preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the mean absolute error of each model is around 15000 to 20000, that means that the prediction of the model is around 15000 to 20000 from the real sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute error of the blened method: 14657.154068785994\n"
     ]
    }
   ],
   "source": [
    "model_lgb.fit(X_train_s, y_train_s)\n",
    "model_ridge.fit(X_train_s, y_train_s)\n",
    "model_cat.fit(X_train_s, y_train_s)\n",
    "\n",
    "# xgb_preds =model_xgb.predict(test_pre)\n",
    "lgb_preds = model_lgb.predict(X_test_s)\n",
    "ridge_preds = model_ridge.predict(X_test_s)\n",
    "cat_preds = model_cat.predict(X_test_s)\n",
    "\n",
    "preds = (lgb_preds + ridge_preds + cat_preds)/3\n",
    "\n",
    "print(f'mean absolute error of the blened method: {metrics.mean_absolute_error(np.exp(y_test_s), np.exp(preds))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the final model (up to this day) can predict the house sale price with a prediction that is on the mean 145657 dollars of the real sale price (only based on the train set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use LightGBM to make a submission\n",
    "> used the following code to make predictions for multiple algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train on train data\n",
    "model_lgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_preds = model_lgb.predict(test_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['Id'] = Id\n",
    "# transform log of SalePrice\n",
    "submission['SalePrice'] = np.exp(lgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('sub_lgb_basis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>116432.185670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>164286.503329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>178339.773554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>182750.515072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>196009.478068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>72843.666095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>82528.469590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>166777.342132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>109135.897066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>228031.570042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  116432.185670\n",
       "1     1462  164286.503329\n",
       "2     1463  178339.773554\n",
       "3     1464  182750.515072\n",
       "4     1465  196009.478068\n",
       "...    ...            ...\n",
       "1454  2915   72843.666095\n",
       "1455  2916   82528.469590\n",
       "1456  2917  166777.342132\n",
       "1457  2918  109135.897066\n",
       "1458  2919  228031.570042\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Model Blending\n",
    "> use XGBRegressor, LGBMRegressor and Ridge Regression to make a prediction, and calculate the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x7ffc5c1f2790>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_xgb.fit(x_train, y_train)\n",
    "model_lgb.fit(x_train, y_train)\n",
    "model_ridge.fit(x_train, y_train)\n",
    "model_cat.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_preds =model_xgb.predict(test_pre)\n",
    "lgb_preds = model_lgb.predict(test_pre)\n",
    "ridge_preds = model_ridge.predict(test_pre)\n",
    "cat_preds = model_cat.predict(test_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (lgb_preds + ridge_preds + cat_preds)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['Id'] = Id\n",
    "# transform log of SalePrice\n",
    "submission['SalePrice'] = np.exp(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('blended_model_basis1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>119147.926766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>163339.278479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>179886.042188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>189303.193779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>194824.591609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>74246.370232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>83536.109919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>172377.871315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>110521.158742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>224917.501842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  119147.926766\n",
       "1     1462  163339.278479\n",
       "2     1463  179886.042188\n",
       "3     1464  189303.193779\n",
       "4     1465  194824.591609\n",
       "...    ...            ...\n",
       "1454  2915   74246.370232\n",
       "1455  2916   83536.109919\n",
       "1456  2917  172377.871315\n",
       "1457  2918  110521.158742\n",
       "1458  2919  224917.501842\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "> use GridSearch to find the best values for the parameter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression\n",
    "- alpha: Regularization strength\n",
    "- fit_intercept: whether to fit the intercept for this model\n",
    "- solver: for the computational routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_Ridge = {'alpha': [5, 1, 0.1], \n",
    "                'fit_intercept': [True, False],\n",
    "                'solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  96 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-2)]: Done 120 out of 120 | elapsed:    4.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True,\n",
       "                             max_iter=None, normalize=False, random_state=None,\n",
       "                             solver='auto', tol=0.001),\n",
       "             iid='warn', n_jobs=-2,\n",
       "             param_grid={'alpha': [5, 1, 0.1], 'fit_intercept': [True, False],\n",
       "                         'solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_log_error', verbose=1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ridge = Ridge()\n",
    "Ridge_gs = GridSearchCV(model_ridge, params_Ridge, scoring='neg_mean_squared_log_error', n_jobs=-2, cv=5, verbose=1)\n",
    "Ridge_gs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1, 'fit_intercept': True, 'solver': 'svd'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ridge_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM\n",
    "- boosting_type: choose Gradient Boosting method\n",
    "- num_leaves: maximum tree leaves for base learners (higher numbers can cause overfitting)\n",
    "- learning_rate: learning rate of Boosting algo\n",
    "- n_estimators: number of boosted trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb = {'boosting_type': ['gbdt', 'dart'],\n",
    "              'num_leaves': [31, 40, 50],\n",
    "              'learning_rate': [0.1, 0.01],\n",
    "              'n_estimators': [100, 150]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-2)]: Done 120 out of 120 | elapsed:   22.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
       "                                     colsample_bytree=1.0,\n",
       "                                     importance_type='split', learning_rate=0.1,\n",
       "                                     max_depth=-1, min_child_samples=20,\n",
       "                                     min_child_weight=0.001, min_split_gain=0.0,\n",
       "                                     n_estimators=100, n_jobs=-1, num_leaves=31,\n",
       "                                     objective=None, random_state=None,\n",
       "                                     reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "                                     subsample=1.0, subsample_for_bin=200000,\n",
       "                                     subsample_freq=0),\n",
       "             iid='warn', n_jobs=-2,\n",
       "             param_grid={'boosting_type': ['gbdt', 'dart'],\n",
       "                         'learning_rate': [0.1, 0.01],\n",
       "                         'n_estimators': [100, 150],\n",
       "                         'num_leaves': [31, 40, 50]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_log_error', verbose=1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modl_lgb = LGBMRegressor()\n",
    "LightGBM_gs = GridSearchCV(model_lgb, params_lgb, scoring='neg_mean_squared_log_error', n_jobs=-2, cv=5, verbose=1)\n",
    "LightGBM_gs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'learning_rate': 0.1,\n",
       " 'n_estimators': 100,\n",
       " 'num_leaves': 31}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LightGBM_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost\n",
    "- iterations: maximum number of trees that can be build\n",
    "- learning_rate: specifies the gradient step\n",
    "- l2_leaf_reg: coefficients for the regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_cat ={'iterations': [50, 100, 150],\n",
    "             'learning_rate': [0.1, 0.05, 0.01],\n",
    "             'l2_leaf_reg': [1, 5, 9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-2)]: Done 135 out of 135 | elapsed:   58.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=<catboost.core.CatBoostRegressor object at 0x7ffc5c3f67d0>,\n",
       "             iid='warn', n_jobs=-2,\n",
       "             param_grid={'iterations': [50, 100, 150], 'l2_leaf_reg': [1, 5, 9],\n",
       "                         'learning_rate': [0.1, 0.05, 0.01]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_log_error', verbose=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cat = CatBoostRegressor(verbose=False)\n",
    "cat_gs = GridSearchCV(model_cat, params_cat, scoring='neg_mean_squared_log_error', n_jobs=-2, cv=5, verbose=1)\n",
    "cat_gs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iterations': 150, 'l2_leaf_reg': 1, 'learning_rate': 0.1}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check tuned models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_lgb rmsle score:\n",
      "0.13562651988770133\n",
      "##############################\n",
      "model_cat rmsle score:\n",
      "0.12674627552051065\n",
      "##############################\n",
      "model_ridge rmsle score:\n",
      "0.12196660786631104\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "# define models\n",
    "model_ridge = Ridge(alpha=1, fit_intercept=True, solver='svd')\n",
    "model_lgb = LGBMRegressor(booting_type='gbdt', learning_rate=0.1, n_estimators=100, num_leaves=31)\n",
    "model_cat = CatBoostRegressor(iterations=150, l2_leaf_reg=1, learning_rate=0.1, verbose=False)\n",
    "\n",
    "# make list for name and model\n",
    "models_list = [model_lgb, model_cat, model_ridge]\n",
    "model_names = ['model_lgb', 'model_cat', 'model_ridge']\n",
    "\n",
    "# print result\n",
    "for model, name in zip(models_list, model_names):\n",
    "    print(name + ' rmsle score:')\n",
    "    print(np.mean(rmsle_cv(model)))\n",
    "    print('#'*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
